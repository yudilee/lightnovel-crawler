import gzip
import hashlib
import importlib.util
import io
import logging
import shutil
from pathlib import Path
from typing import Type

from ...context import ctx
from ...core.crawler import Crawler
from ...utils.url_tools import validate_url
from .dto import CrawlerIndex, CrawlerInfo

logger = logging.getLogger(__name__)


def load_source(file: Path) -> CrawlerIndex:
    json_str = file.read_text(encoding='utf-8')
    return CrawlerIndex.model_validate_json(json_str)


def save_source(file: Path, content: CrawlerIndex):
    file.parent.mkdir(parents=True, exist_ok=True)
    json_str = content.model_dump_json(indent=2)
    file.write_text(json_str, encoding='utf-8')


def fetch_online_source() -> CrawlerIndex:
    compressed = ctx.http.get(ctx.config.crawler.index_file_download_url)
    with gzip.GzipFile(fileobj=io.BytesIO(compressed), mode='rb') as fp:
        json_str = fp.read().decode()
        return CrawlerIndex.model_validate_json(json_str)


def load_offline_source() -> CrawlerIndex:
    # get local index
    local_file = ctx.config.crawler.local_index_file
    local_index = load_source(local_file)

    # get user index. use local index if not available
    user_file = ctx.config.crawler.user_index_file
    if not user_file.is_file():
        user_file.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(local_file, user_file)
        return local_index
    user_index = load_source(user_file)

    # check latest index. use local index if it is latest
    if user_index.v < local_index.v:
        shutil.copy2(local_file, user_file)
        return local_index

    return user_index


def can_do(crawler: Type[Crawler], prop_name: str):
    '''Checks if crawler has implemented the given property name'''
    if not hasattr(crawler, prop_name):
        return False
    if not hasattr(Crawler, prop_name):
        return True
    return getattr(crawler, prop_name) != getattr(Crawler, prop_name)


def has_method(crawler: Type[Crawler], method: str):
    '''Checks if crawler has a callable method'''
    return hasattr(crawler, method) and callable(getattr(crawler, method))


def batch_import_crawlers(*files: Path):
    return (
        crawler
        for file in files
        if file.is_file()
        for crawler in import_crawlers(file)
    )


def import_crawlers(file: Path):
    # validate the file
    if not file.is_file():
        return
    file = file.absolute()
    if file.name.startswith('_'):
        return

    # import modules from the file
    try:
        mod_name = hashlib.md5(file.name.encode()).hexdigest()
        spec = importlib.util.spec_from_file_location(mod_name, file)
        if not (spec and spec.loader):
            logger.info(f"[{file}] Unexpected spec")
            return
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
    except Exception as e:
        logger.info(f"[{file}] Failed to load: {repr(e)}")
        return

    # import all valid crawlers
    for key in dir(module):
        crawler = getattr(module, key)

        # type checks
        if (
            crawler is Crawler
            or type(crawler) is not type(Crawler)
            or not issubclass(crawler, Crawler)
            or crawler.__dict__.get("is_template")
            or getattr(crawler, '__module__', '') != mod_name
        ):
            continue

        # required method checks
        if not has_method(crawler, 'read_novel_info'):
            logger.info(f"[{file}] Missing required method 'read_novel_info'")
            continue
        if not has_method(crawler, 'download_chapter_body'):
            logger.info(f"[{file}] Missing required method 'download_chapter_body'")
            continue

        # base url checks
        base_url = getattr(crawler, "base_url", [])
        urls = [base_url] if isinstance(base_url, str) else base_url
        urls = [str(url).lower().strip("/") + "/" for url in urls]
        urls = [url for url in set(urls) if validate_url(url)]
        if not urls:
            logger.info(f"[{file}] No base url: {crawler}")
            continue

        # static values
        setattr(crawler, "base_url", urls)
        setattr(crawler, "can_login", can_do(crawler, 'login'))
        setattr(crawler, "can_search", can_do(crawler, 'search_novel'))

        # other metdata
        stat = file.stat()
        id = hashlib.md5(str(crawler).encode()).hexdigest()
        setattr(crawler, "__id__", id)
        setattr(crawler, "__file__", str(file))
        setattr(crawler, "version", max(stat.st_mtime, stat.st_ctime))

        yield crawler


def create_crawler_info(crawler: Type[Crawler]):
    file = getattr(crawler, '__file__')
    return CrawlerInfo(
        file_path=file,
        id=getattr(crawler, '__id__'),
        md5=getattr(crawler, '__module__'),
        version=int(getattr(crawler, 'version')),
        base_urls=getattr(crawler, 'base_url'),
        has_mtl=getattr(crawler, 'has_mtl'),
        has_manga=getattr(crawler, 'has_manga'),
        can_login=getattr(crawler, 'can_login'),
        can_search=getattr(crawler, 'can_search'),
        url=f"file:///{Path(file).resolve().as_posix()}",
    )
